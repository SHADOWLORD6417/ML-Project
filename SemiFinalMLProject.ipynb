{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB-BoWviCu4d"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from joblib import dump\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'profile_Dataset.csv'  # Replace with the path to your dataset\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Preprocessing the data\n",
    "# Handle missing values (e.g., fill with median for numerical columns)\n",
    "data.fillna(data.median(numeric_only=True), inplace=True)\n",
    "\n",
    "# Encode categorical variables (if any)\n",
    "for column in data.select_dtypes(include='object').columns:\n",
    "    le = LabelEncoder()\n",
    "    data[column] = le.fit_transform(data[column])\n",
    "\n",
    "# Normalize numerical features\n",
    "scaler = MinMaxScaler()\n",
    "numerical_columns = data.select_dtypes(include=np.number).columns.drop('fake')  # 'fake' is the target\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n",
    "\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = data.drop(columns=['fake']).to_numpy()  # Features\n",
    "y = data['fake'].to_numpy()  # Target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "class KNNMODEL:\n",
    "    def _init_(self):\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.k = None  # Store k as a class attribute\n",
    "\n",
    "    @staticmethod\n",
    "    def euclidean_distance(x1, x2):\n",
    "        differences = x1 - x2\n",
    "        squared_differences = differences ** 2\n",
    "        summed_squares = np.sum(squared_differences, axis=1)\n",
    "        distance = np.sqrt(summed_squares)\n",
    "        return distance\n",
    "\n",
    "    def fit(self, X_train, y_train, k):\n",
    "        \"\"\" Store the training data and k value \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.k = k  # Store k value\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\" Make predictions using the stored training data and k value \"\"\"\n",
    "        predictions = []\n",
    "        for test_point in X_test:\n",
    "            distances = self.euclidean_distance(self.X_train, test_point)\n",
    "            sorted_indices = np.argsort(distances)\n",
    "            nearest_neighbors = sorted_indices[:self.k]  # Use the stored k value\n",
    "            neighbor_labels = self.y_train[nearest_neighbors]\n",
    "            most_common_label = np.bincount(neighbor_labels).argmax()\n",
    "            predictions.append(most_common_label)\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def k_fold_cross_validation(self, X, y, k_folds, k_values):\n",
    "        fold_size = len(X) // k_folds\n",
    "        avg_accuracies = []\n",
    "        fold_results = []\n",
    "        avg_squared_errors = []\n",
    "\n",
    "        for k_val in k_values:\n",
    "            accuracies = []\n",
    "            squared_errors = []\n",
    "            for fold in range(k_folds):\n",
    "                start_index = fold * fold_size\n",
    "                end_index = (fold + 1) * fold_size\n",
    "                X_test = X[start_index:end_index]\n",
    "                y_test = y[start_index:end_index]\n",
    "                X_train = np.concatenate([X[:start_index], X[end_index:]], axis=0)\n",
    "                y_train = np.concatenate([y[:start_index], y[end_index:]], axis=0)\n",
    "\n",
    "                # Fit the model on the training data and store k\n",
    "                self.fit(X_train, y_train, k_val)\n",
    "\n",
    "                y_pred = self.predict(X_test)\n",
    "                accuracy = np.mean(y_pred == y_test)\n",
    "                accuracies.append(accuracy)\n",
    "                squared_error = np.sum((y_pred - y_test) ** 2)\n",
    "                squared_errors.append(squared_error)\n",
    "\n",
    "            avg_accuracies.append(np.mean(accuracies))\n",
    "            avg_squared_errors.append(np.mean(squared_errors))\n",
    "            fold_results.append(accuracies)\n",
    "\n",
    "        overall_avg_accuracy = np.mean(avg_accuracies)\n",
    "        overall_avg_squared_error = np.mean(avg_squared_errors)\n",
    "        return avg_accuracies, fold_results, avg_squared_errors, overall_avg_accuracy, overall_avg_squared_error\n",
    "\n",
    "\n",
    "# Initialize KNN model\n",
    "knn = KNNMODEL()\n",
    "\n",
    "# Define parameters\n",
    "k_values = [1, 3, 5, 7, 9]\n",
    "k_folds = 5\n",
    "\n",
    "# Using k-fold cross-validation\n",
    "avg_accuracies, fold_results, avg_squared_errors, overall_avg_accuracy_k_fold, overall_avg_squared_error_k_fold = knn.k_fold_cross_validation(X, y, k_folds, k_values)\n",
    "\n",
    "# KNN on the split data (Train and Test)\n",
    "knn_accuracies = []\n",
    "knn_squared_errors = []\n",
    "for k in k_values:\n",
    "    # Fit the model on the entire dataset and store k\n",
    "    knn.fit(X, y, k)\n",
    "    y_pred = knn.predict(X)  # Using the entire dataset to predict on itself\n",
    "    accuracy = np.mean(y_pred == y)\n",
    "    knn_accuracies.append(accuracy)\n",
    "    squared_error = np.sum((y_pred - y) ** 2)\n",
    "    knn_squared_errors.append(squared_error)\n",
    "\n",
    "# Saving the trained KNN model\n",
    "\n",
    "dump(knn,'KNN.joblib')\n",
    "\n",
    "\n",
    "# Print results\n",
    "results = {\n",
    "    \"k_values\": k_values,\n",
    "    \"k_fold_squared_errors\": avg_squared_errors,\n",
    "    \"knn_squared_errors\": knn_squared_errors,\n",
    "    \"k_fold_accuracies\": avg_accuracies,\n",
    "    \"knn_accuracies\": knn_accuracies,\n",
    "    \"overall_avg_accuracy_k_fold\": overall_avg_accuracy_k_fold,\n",
    "    \"overall_avg_squared_error_k_fold\": overall_avg_squared_error_k_fold\n",
    "}\n",
    "\n",
    "print(\"------------------------>Results for KNN Classifier<-------------------------------------\")\n",
    "print(\"K Values are:\", k_values)\n",
    "print(\"Overall KNN Squared Errors for each k:\", knn_squared_errors)\n",
    "print(\"Overall KNN Accuracies for each k:\", knn_accuracies)\n",
    "print(\"\\n\\n-------------------->Results for K-FOLD CROSS VALIDATION<---------------------------\")\n",
    "print(\"K-Fold Squared Errors for each k:\", avg_squared_errors)\n",
    "print(\"Overall Average Squared Error for k-Fold Cross Validation:\", overall_avg_squared_error_k_fold)\n",
    "print(\"K-Fold Accuracies for each k:\", avg_accuracies)\n",
    "print(\"Overall Average Accuracy for k-Fold Cross Validation:\", overall_avg_accuracy_k_fold)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \"\"\"Calculate Gini Impurity for a set of labels.\"\"\"\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    probabilities = counts / len(y)\n",
    "    return 1 - np.sum(probabilities**2)\n",
    "\n",
    "def split_dataset(X, y, feature_idx, threshold):\n",
    "    \"\"\"Split the dataset based on a feature and its threshold.\"\"\"\n",
    "    left_mask = X[:, feature_idx] <= threshold\n",
    "    right_mask = ~left_mask\n",
    "    return X[left_mask], X[right_mask], y[left_mask], y[right_mask]\n",
    "\n",
    "\n",
    "def best_split(X, y):\n",
    "    \"\"\"Find the best feature and threshold to split the dataset.\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    best_feature, best_threshold, best_gini = None, None, float('inf')\n",
    "\n",
    "    for feature_idx in range(n_features):\n",
    "        thresholds = np.unique(X[:, feature_idx])\n",
    "        for threshold in thresholds:\n",
    "            _, _, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "\n",
    "            gini = (len(y_left) / n_samples) * gini_impurity(y_left) + \\\n",
    "                   (len(y_right) / n_samples) * gini_impurity(y_right)\n",
    "\n",
    "            if gini < best_gini:\n",
    "                best_feature, best_threshold, best_gini = feature_idx, threshold, gini\n",
    "\n",
    "    return best_feature, best_threshold\n",
    "\n",
    "# Decision Tree Class\n",
    "class DecisionTree:\n",
    "    \"\"\"A simple implementation of a decision tree.\"\"\"\n",
    "    def __init__(self, max_depth=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.tree = None\n",
    "\n",
    "    def fit(self, X, y, depth=0):\n",
    "        \"\"\"Build the decision tree recursively.\"\"\"\n",
    "        if len(np.unique(y)) == 1 or (self.max_depth is not None and depth >= self.max_depth):\n",
    "            # Handle empty subsets or exceeding depth\n",
    "            if len(y) == 0:\n",
    "                return 0  # Return a default value (e.g., 0) if the subset is empty\n",
    "            else:\n",
    "                # Ensure y contains only non-negative values\n",
    "                y_non_negative = np.where(y < 0, 0, y)\n",
    "                return np.bincount(y_non_negative).argmax()\n",
    "\n",
    "        feature_idx, threshold = best_split(X, y)\n",
    "        if feature_idx is None:\n",
    "            return np.bincount(y).argmax()\n",
    "\n",
    "        X_left, X_right, y_left, y_right = split_dataset(X, y, feature_idx, threshold)\n",
    "        node = {\"feature\": feature_idx, \"threshold\": threshold}\n",
    "        node[\"left\"] = self.fit(X_left, y_left, depth + 1)\n",
    "        node[\"right\"] = self.fit(X_right, y_right, depth + 1)\n",
    "        return node\n",
    "\n",
    "    def predict_sample(self, node, x):\n",
    "        \"\"\"Predict the class for a single sample.\"\"\"\n",
    "        if isinstance(node, dict):\n",
    "            if x[node[\"feature\"]] <= node[\"threshold\"]:\n",
    "                return self.predict_sample(node[\"left\"], x)\n",
    "            else:\n",
    "                return self.predict_sample(node[\"right\"], x)\n",
    "        return node\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict classes for multiple samples.\"\"\"\n",
    "        return np.array([self.predict_sample(self.tree, x) for x in X])\n",
    "\n",
    "# Random Forest Class\n",
    "class RandomForest:\n",
    "    \"\"\"A simple implementation of a Random Forest classifier.\"\"\"\n",
    "    def __init__(self, n_trees=10, max_depth=None):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def bootstrap_sample(self, X, y):\n",
    "        \"\"\"Generate a bootstrap sample of the dataset.\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        indices = np.random.choice(n_samples, n_samples, replace=True)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit multiple decision trees on bootstrap samples.\"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            X_sample, y_sample = self.bootstrap_sample(X, y)\n",
    "            tree = DecisionTree(max_depth=self.max_depth)\n",
    "            tree.tree = tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Get predictions from individual trees\n",
    "        tree_predictions = np.array([tree.predict(X) for tree in self.trees])\n",
    "\n",
    "        # Ensure tree_predictions are integers\n",
    "        tree_predictions = tree_predictions.astype(int)\n",
    "\n",
    "        # Ensure predictions are non-negative\n",
    "        tree_predictions[tree_predictions < 0] = 0  # Or handle negative values appropriately\n",
    "\n",
    "        if tree_predictions.size == 0:\n",
    "          raise ValueError(\"No predictions available the trees.\")\n",
    "\n",
    "        # Apply bincount and argmax\n",
    "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=tree_predictions)\n",
    "\n",
    "\n",
    "\n",
    "# Train the Decision Tree model\n",
    "tree = DecisionTree(max_depth=5)  # You can choose a different max_depth if needed\n",
    "tree.tree = tree.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "tree_predictions = tree.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "tree_accuracy = np.mean(tree_predictions == y_test)\n",
    "print(f\"Decision Tree Accuracy: {tree_accuracy:.2f}\")\n",
    "\n",
    "# Save the trained random forest model\n",
    "\n",
    "# Train the Random Forest model\n",
    "forest = RandomForest(n_trees=10, max_depth=5)\n",
    "forest.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "forest_predictions = forest.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = np.mean(forest_predictions == y_test)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "# Save the trained random forest model\n",
    "dump(forest, 'random_forest_model.joblib')\n",
    "print(\"Random Forest model saved as 'random_forest_model.joblib'\")\n",
    "\n",
    "\n",
    "# Accuracy comparison bar chart\n",
    "models = ['Decision Tree', 'Random Forest']\n",
    "accuracies = [tree_accuracy, accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=list(models), y=list(accuracies), palette=\"viridis\")  # Explicitly convert to list\n",
    "plt.title(\"Model Accuracy Comparison\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.ylim(0, 1)  # Accuracy ranges from 0 to 1\n",
    "for i, acc in enumerate(accuracies):\n",
    "    plt.text(i, acc + 0.01, f\"{acc:.2f}\", ha='center', va='bottom')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6_fFiH_Dro-"
   },
   "outputs": [],
   "source": [
    "# Manual Gradient Boosting Implementation\n",
    "class ManualGradientBoosting:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):  # Fixed '__init__' typo\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "        self.initial_mean = 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Convert binary labels to {-1, 1}\n",
    "        y = np.where(y == 0, -1, 1)\n",
    "\n",
    "        # Initial prediction: mean of target labels\n",
    "        self.initial_mean = np.mean(y)\n",
    "\n",
    "        predictions = np.full(y.shape, self.initial_mean)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals (pseudo-residuals)\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Train a weak learner (decision tree regressor) on the residuals\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            self.models.append(model)\n",
    "\n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Start with the initial mean for all samples\n",
    "        predictions = np.full(X.shape[0], self.initial_mean)\n",
    "\n",
    "        # Add contributions from weak learners\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "\n",
    "        # Convert to binary labels\n",
    "        return np.where(predictions >= 0, 1, 0)\n",
    "\n",
    "# Train and evaluate Manual Gradient Boosting\n",
    "manual_gb = ManualGradientBoosting(n_estimators=50, learning_rate=0.1, max_depth=3)\n",
    "manual_gb.fit(X_train, y_train)\n",
    "manual_gb_predictions = manual_gb.predict(X_test)\n",
    "manual_gb_accuracy = accuracy_score(y_test, manual_gb_predictions)\n",
    "\n",
    "# Train and evaluate DecisionTreeRegressor\n",
    "decision_tree = DecisionTreeRegressor(max_depth=3)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "decision_tree_predictions = decision_tree.predict(X_test)\n",
    "decision_tree_accuracy = accuracy_score(y_test, np.round(decision_tree_predictions))  # Round predictions for classification\n",
    "\n",
    "# Save the trained random forest model\n",
    "dump(decision_tree, 'decision_model.joblib')\n",
    "\n",
    "# Print accuracies\n",
    "print(f\"Manual Gradient Boosting Accuracy: {manual_gb_accuracy:.2f}\")\n",
    "print(f\"Decision Tree Regressor Accuracy: {decision_tree_accuracy:.2f}\")\n",
    "\n",
    "# Plotting the accuracies\n",
    "models = ['DecisionTreeRegressor', 'ManualGradientBoosting']\n",
    "accuracies = [decision_tree_accuracy, manual_gb_accuracy]\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar(models, accuracies, color=['skyblue', 'lightgreen'])\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yekxawWRDhva"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Simple SVM Class Implementation\n",
    "class SimpleSVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.n_iters = n_iters\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        self.b = 0\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    self.w -= self.lr * (2 * self.lambda_param * self.w)\n",
    "                else:\n",
    "                    self.w -= self.lr * (\n",
    "                        2 * self.lambda_param * self.w - np.dot(x_i, y[idx])\n",
    "                    )\n",
    "                    self.b -= self.lr * y[idx]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(np.dot(X, self.w) - self.b)\n",
    "\n",
    "\n",
    "# Function to normalize the dataset\n",
    "def normalize_data(X):\n",
    "    return (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "\n",
    "# Function to compute PCA manually\n",
    "def pca(X, n_components=2):\n",
    "    # Step 1: Compute the covariance matrix\n",
    "    covariance_matrix = np.cov(X.T)\n",
    "\n",
    "    # Step 2: Compute eigenvalues and eigenvectors\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "    # Step 3: Sort eigenvectors by descending eigenvalues\n",
    "    sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "    top_eigenvectors = eigenvectors[:, sorted_indices[:n_components]]\n",
    "\n",
    "    # Step 4: Project data onto the top eigenvectors\n",
    "    reduced_data = np.dot(X, top_eigenvectors)\n",
    "\n",
    "    return reduced_data\n",
    "\n",
    "\n",
    "# Function to split the dataset\n",
    "def train_test_split(X, y, test_size=0.2):\n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    # Calculate the split index\n",
    "    split_index = int((1 - test_size) * X.shape[0])\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test = X[:split_index], X[split_index:]\n",
    "    y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "\n",
    "# Function to plot the decision boundary in 2D\n",
    "def plot_decision_boundary_2d(X, y, svm, reduced_X):\n",
    "    plt.scatter(\n",
    "        reduced_X[:, 0], reduced_X[:, 1], c=y, cmap=\"coolwarm\", s=30, edgecolors=\"k\"\n",
    "    )\n",
    "\n",
    "    # Decision boundary in reduced space\n",
    "    x0 = np.linspace(reduced_X[:, 0].min(), reduced_X[:, 0].max(), 100)\n",
    "    x1 = (\n",
    "        -(svm.w[0] * x0 + svm.b) / svm.w[1]\n",
    "    )  # Project the high-dimensional weights onto 2D space\n",
    "    plt.plot(x0, x1, color=\"black\", linewidth=2, label=\"Decision Boundary\")\n",
    "\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.legend()\n",
    "    plt.title(\"SVM Decision Boundary with All Features (Manual PCA)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Load the dataset\n",
    "    # Replace 'profile_Dataset.csv' with the actual file path\n",
    "data = pd.read_csv(\"profile_Dataset.csv\")\n",
    "\n",
    "# Features and target selection\n",
    "X = data.drop(columns=[\"fake\"]).values  # All attributes except the target\n",
    "y = data[\"fake\"].values  # Target variable\n",
    "\n",
    "# Encode the target variable (if not already binary: e.g., 0/1)\n",
    "y = np.where(y == 1, 1, -1)\n",
    "\n",
    "# Normalize the data\n",
    "X = normalize_data(X)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Train the SVM\n",
    "svm = SimpleSVM()\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# Calculate and print accuracy\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(f\"Accuracy on test set: {acc:.2f}\")\n",
    "\n",
    "# Reduce dimensions for plotting using manual PCA\n",
    "X_reduced = pca(X_train, n_components=2)\n",
    "\n",
    "# Plot the decision boundary in 2D\n",
    "plot_decision_boundary_2d(X_train, y_train, svm, X_reduced)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LAQHK-yoHfl3",
    "outputId": "0c42d00d-ac10-4fb0-8221-4f98826ec35f"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    print(\"Please enter the values for the following features:\")\n",
    "    # Get individual inputs for each feature (13 features, including 'follows')\n",
    "    profile_pic = float(input(\"Profile Picture (1 if present, 0 if absent): \"))\n",
    "    username_length = float(input(\"Username Length (number of characters): \"))\n",
    "    username_nums = float(input(\"Numerical Count in Username: \"))\n",
    "    fullname_length = float(input(\"Full Name Length (number of characters): \"))\n",
    "    fullname_words = float(input(\"Number of Words in Full Name: \"))\n",
    "    name_equals_username = float(input(\"Name equals Username (1 if yes, 0 if no): \"))\n",
    "    description_length = float(input(\"Description Length (number of characters): \"))\n",
    "    external_url = float(input(\"External URL (1 if present, 0 if absent): \"))\n",
    "    private = float(input(\"Private Account (1 if private, 0 if public): \"))\n",
    "    num_posts = float(input(\"Number of Posts: \"))\n",
    "    num_followers = float(input(\"Number of Followers: \"))\n",
    "    num_follows = float(input(\"Number of Follows: \"))\n",
    "\n",
    "    # Transformed features for the model\n",
    "    second_feature = (\n",
    "        username_nums / username_length if username_length != 0 else 0\n",
    "    )  # Avoid division by zero\n",
    "    fourth_feature = (\n",
    "        username_nums / fullname_length if fullname_length != 0 else 0\n",
    "    )  # Avoid division by zero\n",
    "\n",
    "    # Prepare the user data as input for the model (including transformed features)\n",
    "    user_data = [\n",
    "        profile_pic,  # 1st feature\n",
    "        second_feature,  # 2nd feature (nums/length of username)\n",
    "        fullname_length,  # 3rd feature (original full name length)\n",
    "        fourth_feature,  # 4th feature (nums/length of full name)\n",
    "        name_equals_username,  # 5th feature\n",
    "        description_length,  # 6th feature\n",
    "        external_url,  # 7th feature\n",
    "        private,  # 8th feature\n",
    "        num_posts,  # 9th feature\n",
    "        num_followers,  # 10th feature\n",
    "        num_follows,  # 11th fe11ature\n",
    "    ]\n",
    "\n",
    "    # Display predictions for each model\n",
    "    user_data = np.array(user_data).reshape(1, -1)\n",
    "\n",
    "    # Make predictions for each model\n",
    "    print(\"\\n--- Predictions ---\")\n",
    "\n",
    "    model1 = forest\n",
    "    prediction = model1.predict(user_data)  # Ensure the shape is (1, n_features)\n",
    "    print(f\"{model1.__class__.__name__}: {'Fake' if prediction[0] else 'Real'} Account\")\n",
    "\n",
    "    model2 = svm\n",
    "    prediction = model2.predict(user_data)  # Ensure the shape is (1, n_features)\n",
    "    print(f\"{model2.__class__.__name__}: {'Fake' if prediction[0] else 'Real'} Account\")\n",
    "\n",
    "    model3 = decision_tree\n",
    "    prediction = model3.predict(user_data)  # Ensure the shape is (1, n_features)\n",
    "    print(f\"{model3.__class__.__name__}: {'Fake' if prediction[0] else 'Real'} Account\")\n",
    "\n",
    "    model4 = knn\n",
    "    prediction = model4.predict(user_data)  # Ensure the shape is (1, n_features)\n",
    "    print(f\"{model4.__class__.__name__}: {'Fake' if prediction[0] else 'Real'} Account\")\n",
    "\n",
    "    # Ask if the user wants to continue or stop\n",
    "    continue_input = input(\"\\nDo you want to enter new data? (y/n): \").lower()\n",
    "    if continue_input != \"y\":\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
